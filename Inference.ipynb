{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890819c4-511d-4370-89a5-1180bed3da8e",
   "metadata": {},
   "source": [
    "# Inference \n",
    "\n",
    "This notebook demonstrates how to use your custom Transformer model for inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cae167c7-f31e-4b64-a555-c87205653dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules, including custom Transformer class, PyTorch utilities, and the HuggingFace tokenizer.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from my_transformers.Transformers_archi import Transformer \n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fc81865-0e7d-4f19-a279-6a07a486ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of tiny English-French parallel dataset\n",
    "train_data = [\n",
    "    (\"Hello\", \"Bonjour\"),\n",
    "    (\"How are you?\", \"Comment ça va?\"),\n",
    "    (\"I am fine\", \"Je vais bien\"),\n",
    "    (\"What is your name?\", \"Quel est ton nom?\"),\n",
    "    (\"I love programming\", \"J'adore la programmation\"),\n",
    "    (\"This is a test\", \"C'est un test\"),\n",
    "    (\"Good morning\", \"Bonjour\"),\n",
    "    (\"Good night\", \"Bonne nuit\"),\n",
    "    (\"I need help\", \"J'ai besoin d'aide\"),\n",
    "    (\"Thank you\", \"Merci\"),\n",
    "    (\"Please\", \"S'il vous plaît\"),\n",
    "    (\"Goodbye\", \"Au revoir\"),\n",
    "    (\"Sorry\", \"Désolé\"),\n",
    "    (\"I am happy.\", \"Je suis heureux.\"),\n",
    "    (\"She is reading a book.\", \"Elle lit un livre.\"),\n",
    "    (\"We are going to school.\", \"Nous allons à l'école.\"),\n",
    "    (\"They are playing outside.\", \"Ils jouent dehors.\"),\n",
    "    (\"It is raining.\", \"Il pleut.\"),\n",
    "    (\"What is your name?\", \"Comment tu t'appelles ?\"),\n",
    "    (\"I don't understand.\", \"Je ne comprends pas.\"),\n",
    "    (\"Please help me.\", \"S'il vous plaît, aidez-moi.\"),\n",
    "    (\"I love you.\", \"Je t'aime.\"),\n",
    "    (\"Where is the bathroom?\", \"Où sont les toilettes ?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3890f1d4-5881-485f-a68a-f3c7682155ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset for tokenized source and target sentences\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src_token, trg_tokens):\n",
    "        # Store input IDs and attention masks for source, and input IDs for target\n",
    "        self.src_mask = src_token[\"attention_mask\"]\n",
    "        self.src_ids = src_token[\"input_ids\"]\n",
    "        self.trg_ids = trg_tokens[\"input_ids\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.src_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Return a dictionary with source input IDs, attention mask, and target labels for the given index\n",
    "        return {\n",
    "            \"input_ids\": self.src_ids[index],\n",
    "            \"attention_mask\": self.src_mask[index],\n",
    "            \"labels\": self.trg_ids[index] }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03b2e87-82e3-475e-a423-4a9bef4ab6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# Import HuggingFace's AutoTokenizer pretrained English-French tokenizer for tokenizing sentences\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a527e19a-f3e9-488b-a19e-d9773beb13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals,preds= zip(*train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef3169-a918-4c40-8b26-e5721d7df218",
   "metadata": {},
   "source": [
    "## Tokenize the Data\n",
    "Set a maximum sequence length and tokenize both the actual (source) and predicted (target) sentences, padding or truncating as necessary. The tokens are returned as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f99769-b70f-4747-9e50-b7d15acc2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length for tokenization\n",
    "max_length = 10\n",
    "\n",
    "# Tokenize the actual (input/source) sentences\n",
    "actual_tokens = tokenizer(\n",
    "    actuals,\n",
    "    max_length=max_length,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True)\n",
    "\n",
    "# Tokenize the predicted (target) sentences\n",
    "pred_tokens = tokenizer(\n",
    "    preds,\n",
    "    max_length=max_length,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e5eceb-b324-4dc2-8a12-031311fe4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the tokenized actual and predicted sentences\n",
    "dataset = CustomDataset(actual_tokens, pred_tokens)\n",
    "\n",
    "# Wrap the dataset in a DataLoader for batching and shuffling\n",
    "data = DataLoader(dataset, batch_size= 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f87b93d-e34a-4589-bb29-96ddb9dc0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the custom Transformer model with specified hyperparameters\n",
    "model = Transformer(\n",
    "    d_model=128,\n",
    "    num_heads=2,\n",
    "    num_encoder_layer=4,\n",
    "    src_vocab_size=tokenizer.vocab_size,\n",
    "    trg_vocab_size=tokenizer.vocab_size,\n",
    "    seq_len=max_length,\n",
    "    decoder_layer=2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5056e421-4963-4540-b8ba-d2e2727541f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Adam optimizer for the model's parameters with a learning rate of 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Use CrossEntropyLoss as the loss function for training/evaluation\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a278d92e-73d5-4c45-b5bf-876e881dbf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ---->> LOSS 55.3920\n",
      "Epoch 2 ---->> LOSS 49.2084\n",
      "Epoch 3 ---->> LOSS 53.1253\n",
      "Epoch 4 ---->> LOSS 52.7743\n",
      "Epoch 5 ---->> LOSS 46.5644\n",
      "Epoch 6 ---->> LOSS 46.0022\n",
      "Epoch 7 ---->> LOSS 45.5231\n",
      "Epoch 8 ---->> LOSS 44.1990\n",
      "Epoch 9 ---->> LOSS 44.5378\n",
      "Epoch 10 ---->> LOSS 43.5150\n",
      "Epoch 11 ---->> LOSS 38.9621\n",
      "Epoch 12 ---->> LOSS 39.3133\n",
      "Epoch 13 ---->> LOSS 40.0459\n",
      "Epoch 14 ---->> LOSS 40.2793\n",
      "Epoch 15 ---->> LOSS 37.3698\n",
      "Epoch 16 ---->> LOSS 36.6405\n",
      "Epoch 17 ---->> LOSS 39.4404\n",
      "Epoch 18 ---->> LOSS 36.1151\n",
      "Epoch 19 ---->> LOSS 38.0210\n",
      "Epoch 20 ---->> LOSS 34.8968\n",
      "Total time taken: 1.20 minutes\n"
     ]
    }
   ],
   "source": [
    "# Set the model to training mode\n",
    "model.train()\n",
    "epochs = 20\n",
    "start = time.time()\n",
    "\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in data:\n",
    "        input_ids = batch[\"input_ids\"]           # [B, T]\n",
    "        attention_mask = batch[\"attention_mask\"] # [B, T]\n",
    "        labels = batch[\"labels\"]                 # [B, T]\n",
    "\n",
    "        # Forward pass: output shape [B, T, vocab_size]\n",
    "        output = model(input_ids, attention_mask, labels)\n",
    "\n",
    "        # Permute output for CrossEntropyLoss: [B, vocab_size, T]\n",
    "        output = output.permute(0, 2, 1)  \n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Print loss after each epoch\n",
    "    print(f\"Epoch {e+1} ---->> LOSS {total_loss:.4f}\")\n",
    "\n",
    "# Print total training time\n",
    "print(f\"Total time taken: {(time.time() - start) / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e1868a-57d7-44b1-b5dd-b61a1f6b61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a single source sentence for model input\n",
    "src_sentence = \"hello world\"\n",
    "src_inputs = tokenizer(\n",
    "    src_sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99908be5-a7bf-4afe-893d-da34519dbbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get token ids and attention mask for the source sentence\n",
    "input_ids = src_inputs[\"input_ids\"]\n",
    "attention_mask = src_inputs[\"attention_mask\"]\n",
    "\n",
    "# Pass tokens to the encoder (no gradients needed for inference)\n",
    "with torch.no_grad():\n",
    "    encoder_out = model.enc(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffc84e00-f58b-4af0-bdf6-7ea1b2941ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start decoder input with the pad token (can also be sos token for some models)\n",
    "dec_inp_id = torch.tensor([[tokenizer.pad_token_id]])  # shape: (1, seqlen)\n",
    "\n",
    "for _ in range(max_length):\n",
    "    # Get logits from the decoder\n",
    "    logits = model.dec(dec_inp_id, encoder_out)  # logits: (1, seq_len, vocab_size)\n",
    "\n",
    "    # Pick the most probable next token (greedy decoding)\n",
    "    next_token = torch.argmax(logits[:, -1, :], dim=-1)  # shape: (1,)\n",
    "\n",
    "    # Concatenate the new token to the decoder input\n",
    "    dec_inp_id = torch.cat([dec_inp_id, next_token.unsqueeze(0)], dim=1)\n",
    "    # .unsqueeze(0) ensures next_token shape matches for concatenation\n",
    "\n",
    "    # Stop if end-of-sequence token is generated\n",
    "    if next_token.item() == tokenizer.eos_token_id:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e25ff389-d6ae-4821-a818-c6794b7da05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- de en▁are cette in leur leur de en'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert generated token IDs to a flat list\n",
    "output_tokens = dec_inp_id[0].tolist()\n",
    "\n",
    "# Decode the token IDs to a string, skipping special tokens like <pad> or <eos>\n",
    "decoded_text = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "# The final generated text\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a200e1-42c9-46ce-8d8f-7ad3eb2bd398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
