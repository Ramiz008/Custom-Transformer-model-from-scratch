{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b489371e-7024-4583-800b-3b8f266bd9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85baccf3-9c86-4aec-a534-c0963e6f56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the positional encoding layer as described in the Transformer model.\n",
    "    Adds positional information to token embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, d_model, vocab_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len (int): Maximum length of input sequences.\n",
    "            d_model (int): Dimensionality of the embeddings.\n",
    "            vocab_size (int): Size of the vocabulary for embedding lookup.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create a matrix of [seq_len, d_model] for positional encodings\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1)  # Shape: (seq_len, 1)\n",
    "        \n",
    "        # Compute the div_term for sine/cosine arguments\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices, cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer so it is saved with the model but not a parameter\n",
    "        self.register_buffer(\"pos_enc\", pe.unsqueeze(0))  # Shape: (1, seq_len, d_model)\n",
    "        \n",
    "        # Token embedding layer\n",
    "        self.embedder = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input indices of shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            Tensor: Embedded input + positional encoding, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.embedder(x)  # Shape: (batch_size, seq_len, d_model)\n",
    "        # Add positional encoding (broadcasts over batch dimension)\n",
    "        return x + self.pos_enc[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb400c22-f2c2-4b4e-ba04-e9485764b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Multi-Head Attention mechanism as used in Transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the model (embedding size).\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.q_linear = nn.Linear(d_model, d_model)  # Linear layer for queries\n",
    "        self.k_linear = nn.Linear(d_model, d_model)  # Linear layer for keys\n",
    "        self.v_linear = nn.Linear(d_model, d_model)  # Linear layer for values\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dims = d_model // num_heads\n",
    "        \n",
    "        # Ensure d_model is divisible by num_heads\n",
    "        if self.d_model % self.num_heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by num_heads\")\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query, key, value (Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            mask (Tensor, optional): Mask tensor, shape broadcastable to attention matrix.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output after multi-head attention, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Linear projections\n",
    "        q = self.q_linear(query)\n",
    "        k = self.k_linear(key)\n",
    "        v = self.v_linear(value)\n",
    "        \n",
    "        batch_size, q_len, _ = query.shape\n",
    "        k_len = key.shape[1]\n",
    "        v_len = value.shape[1]\n",
    "        \n",
    "        # Reshape and permute for multi-head attention\n",
    "        # New shape: (batch_size, num_heads, seq_len, head_dims)\n",
    "        q = q.reshape(batch_size, q_len, self.num_heads, self.head_dims).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(batch_size, k_len, self.num_heads, self.head_dims).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(batch_size, v_len, self.num_heads, self.head_dims).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dims)  # (batch, heads, q_len, k_len)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, float('-inf'))  \n",
    "        \n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        attn_out = torch.matmul(attn, v)  # (batch, heads, q_len, head_dims)\n",
    "        \n",
    "        # Concatenate heads and reshape to original dimensions\n",
    "        attn_out = attn_out.permute(0, 2, 1, 3).reshape(batch_size, q_len, self.d_model)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9a5687-eea6-49c8-95f7-ba8ef4b29fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Feed-Forward Network (FFN) used in Transformer blocks.\n",
    "    Applies two linear transformations with a ReLU activation and dropout in between.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model): \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the model (embedding size).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # First linear layer expands dimensionality (e.g., 512 -> 2048 as in original Transformer paper)\n",
    "        self.layer1 = nn.Linear(d_model, d_model * 4)\n",
    "        # Second linear layer projects back to original dimension (e.g., 2048 -> 512)\n",
    "        self.layer2 = nn.Linear(d_model * 4, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.drop(self.relu(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6bda23d-affd-44c4-8080-57f927afd255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single Encoder block as used in the Transformer architecture.\n",
    "    Consists of Multi-Head Attention, followed by Feed-Forward Network with Layer Normalization and residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the model (embedding size).\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)  # Multi-Head Attention layer\n",
    "        self.ffn = FFN(d_model)                                              # Feed-Forward Network\n",
    "        self.norm1 = nn.LayerNorm(d_model)                                   # LayerNorm after attention\n",
    "        self.norm2 = nn.LayerNorm(d_model)                                   # LayerNorm after FFN\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask (Tensor, optional): Mask tensor for attention, shape broadcastable to attention matrix.\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Self-attention (q, k, v are all x in encoder)\n",
    "        attn = self.mha(x, x, x, mask)\n",
    "        # Residual connection + LayerNorm\n",
    "        x = self.norm1(x + attn)\n",
    "        # Feed-Forward Network + Residual connection + LayerNorm\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dadc4f6d-3be4-42b9-92cb-29de028531fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the full Encoder Block consisting of positional encoding and a stack of Encoder layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, num_layers, src_vocab_size, seq_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the model (embedding size).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            num_layers (int): Number of Encoder layers to stack.\n",
    "            src_vocab_size (int): Vocabulary size for source language.\n",
    "            seq_len (int): Maximum input sequence length.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Positional encoding with input embedding\n",
    "        self.pos_enc = PositionalEncoding(seq_len=seq_len, d_model=d_model, vocab_size=src_vocab_size)\n",
    "        \n",
    "        # Stack of Encoder layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(Encoder(d_model=d_model,num_heads=num_heads))\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len)\n",
    "            mask (Tensor, optional): Mask tensor for attention.\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.pos_enc(x)  # Add token embeddings and positional encodings\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)  # Pass through each encoder layer\n",
    "        return self.norm(x)     # Final normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e193ccda-8c03-495e-bc1a-3432f2288544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-Head Self-Attention with causal masking (for decoder blocks in Transformers).\n",
    "    Ensures that each position can only attend to previous positions (leftward/self-masking).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the model (embedding size).\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.q_linear = nn.Linear(d_model, d_model)  # Linear layer for queries\n",
    "        self.k_linear = nn.Linear(d_model, d_model)  # Linear layer for keys\n",
    "        self.v_linear = nn.Linear(d_model, d_model)  # Linear layer for values\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(\"d_model not divisible by num_heads\")\n",
    "            \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask (Tensor, optional): Optional mask tensor (e.g., for padding).\n",
    "        Returns:\n",
    "            Tensor: Output tensor after masked multi-head attention, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Linear projections for Q, K, V\n",
    "        q = self.q_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Reshape and permute for multi-head attention\n",
    "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (batch, heads, seq_len, seq_len)\n",
    "        \n",
    "        # Apply causal (look-ahead) mask if not provided\n",
    "        if mask is None:\n",
    "            mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "            attn = attn.masked_fill(mask == 1, float(\"-inf\"))\n",
    "        \n",
    "        \n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        attn = torch.matmul(attn, v)\n",
    "        \n",
    "        # Reshape back to (batch_size, seq_len, d_model)\n",
    "        out = attn.permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.d_model)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06dc35d4-62fb-46a1-8a60-ecc09d43688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single Decoder block for Transformer models.\n",
    "    Includes masked self-attention, encoder-decoder (cross) attention, and a feed-forward network, each with residual connection and layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.masked_attn = MaskedMultiheadAttention(d_model=d_model, num_heads=num_heads)  # Masked self-attention \n",
    "        self.multihead_cross_attn = MultiHeadAttention(d_model=d_model, num_heads=num_heads)  # Encoder-decoder cross attention\n",
    "        self.ffn = FFN(d_model=d_model)     # Feed-forward network\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Norm after masked attention\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # Norm after cross attention\n",
    "        self.norm3 = nn.LayerNorm(d_model)  # Norm after FFN\n",
    "        self.drop = nn.Dropout(p=0.2)       # Final dropout\n",
    "\n",
    "    def forward(self, x, encoder_out, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Decoder input (batch_size, seq_len, d_model)\n",
    "            encoder_out (Tensor): Encoder output (batch_size, seq_len, d_model)\n",
    "            mask (Tensor, optional): Mask tensor for causal attention\n",
    "        Returns:\n",
    "            Tensor: Output tensor (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Masked self-attention with residual connection and normalization\n",
    "        x1 = self.norm1(x)\n",
    "        attn1 = self.masked_attn(x1, mask)\n",
    "        x = x + attn1\n",
    "        \n",
    "        # Encoder-decoder cross attention (query from decoder, key/value from encoder)\n",
    "        x2 = self.norm2(x)\n",
    "        attn2 = self.multihead_cross_attn(query=x2, key=encoder_out, value=encoder_out)\n",
    "        x = x + attn2\n",
    "        \n",
    "        # Feed-forward network with residual connection and normalization\n",
    "        x3 = self.norm3(x)\n",
    "        ffn_out = self.ffn(x3)\n",
    "        x = x + ffn_out\n",
    "\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33655984-7c25-480b-8faa-bfd225b3505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacks multiple Decoder layers and applies positional encoding for the target sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, d_model, decoder_layer, seq_len, trg_vocab):\n",
    "        super().__init__()\n",
    "        self.pos = PositionalEncoding(seq_len=seq_len, d_model=d_model, vocab_size=trg_vocab)  # Target embedding + position\n",
    "        self.layer = nn.ModuleList()\n",
    "        for _ in range(decoder_layer):\n",
    "            self.layer.append(Decoder(d_model = d_model,num_heads=num_heads))\n",
    "          # Stack of decoder layers\n",
    "                \n",
    "    def forward(self, x, encoder_out, mask=None):\n",
    "        x = self.pos(x)\n",
    "        for layer in self.layer:\n",
    "            x = layer(x, encoder_out, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9cb395d-4e73-4ad5-a223-5ca32f4a8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Full Transformer model with encoder, decoder, and final output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, num_encoder_layer, src_vocab_size, trg_vocab_size,\n",
    "                 seq_len, decoder_layer):\n",
    "        super().__init__()\n",
    "        self.enc = EncoderBlock(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_encoder_layer,\n",
    "            src_vocab_size=src_vocab_size,\n",
    "            seq_len=seq_len\n",
    "        )\n",
    "        self.dec = DecoderBlock(\n",
    "            num_heads=num_heads,\n",
    "            d_model=d_model,\n",
    "            decoder_layer=decoder_layer,\n",
    "            seq_len=seq_len,\n",
    "            trg_vocab=trg_vocab_size\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, trg_vocab_size)  # Output logits for target vocab\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, source, target, mask=None):\n",
    "        enc_out = self.enc(source)\n",
    "        dec_out = self.dec(target, enc_out, mask)\n",
    "        out = self.fc_out(dec_out)\n",
    "        logits = self.dropout(out)\n",
    "        return logits  # (batch, trg_seq_len, trg_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199f54f-db54-4185-b37b-43e3206387fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
